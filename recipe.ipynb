{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:24:04.783359Z",
     "iopub.status.busy": "2025-06-02T21:24:04.783156Z",
     "iopub.status.idle": "2025-06-02T21:24:34.443479Z",
     "shell.execute_reply": "2025-06-02T21:24:34.442687Z",
     "shell.execute_reply.started": "2025-06-02T21:24:04.783343Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zagorulia/nlp/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:24:34.444906Z",
     "iopub.status.busy": "2025-06-02T21:24:34.444384Z",
     "iopub.status.idle": "2025-06-02T21:25:15.397923Z",
     "shell.execute_reply": "2025-06-02T21:25:15.397344Z",
     "shell.execute_reply.started": "2025-06-02T21:24:34.444887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 1. Load and preprocess the CSV\n",
    "# # -----------------------------\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# INPUT_CSV_PATH = \"3A2M_EXTENDED.csv\"\n",
    "OUTPUT_DIR = \"./gpt2-ner2directions\"\n",
    "TRAIN_FILE = \"train_ner2dir.txt\"\n",
    "VAL_FILE = \"val_ner2dir.txt\"\n",
    "\n",
    "# # Load the DataFrame\n",
    "# df = pd.read_csv(INPUT_CSV_PATH)# [:int(1e)]\n",
    "\n",
    "# # Ensure NER and directions columns exist; drop rows missing either\n",
    "# if \"NER\" not in df.columns or \"directions\" not in df.columns:\n",
    "#     raise ValueError(\"CSV must contain 'NER' and 'directions' columns.\")\n",
    "# df = df.dropna(subset=[\"NER\", \"directions\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:25:15.399825Z",
     "iopub.status.busy": "2025-06-02T21:25:15.399626Z",
     "iopub.status.idle": "2025-06-02T21:25:15.411359Z",
     "shell.execute_reply": "2025-06-02T21:25:15.410569Z",
     "shell.execute_reply.started": "2025-06-02T21:25:15.399810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 2. Format examples as text blocks\n",
    "# # -----------------------------\n",
    "# def format_example_from_ner(ner_text: str, directions_text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Combine the NER text and directions text into a single string block:\n",
    "    \n",
    "#     NER:\n",
    "#     - entity1\n",
    "#     - entity2\n",
    "#     ...\n",
    "\n",
    "#     Directions:\n",
    "#     1. step one\n",
    "#     2. step two\n",
    "#     ...\n",
    "    \n",
    "#     Ends with two newlines as a delimiter.\n",
    "#     \"\"\"\n",
    "#     parts = []\n",
    "#     parts.append(\"NER:\")\n",
    "#     # Split NER on commas; adjust if your NER uses newlines or another delimiter\n",
    "#     for ent in ner_text.split(\",\"):\n",
    "#         ent = ent.strip()\n",
    "#         if ent:\n",
    "#             parts.append(f\"- {ent}\")\n",
    "#     parts.append(\"\")  # blank line between sections\n",
    "#     parts.append(\"Directions:\")\n",
    "#     # Split directions on newline; keep existing multi‐line steps\n",
    "#     for idx, step in enumerate(directions_text.split(\"\\n\"), start=1):\n",
    "#         step = step.strip()\n",
    "#         if step:\n",
    "#             parts.append(f\"{idx}. {step}\")\n",
    "#     # Two newlines to separate examples\n",
    "#     return \"\\n\".join(parts) + \"\\n\\n\"\n",
    "\n",
    "# # Build a list of formatted examples\n",
    "# examples = []\n",
    "# for _, row in df.iterrows():\n",
    "#     ner_text = row[\"NER\"]\n",
    "#     dir_text = row[\"directions\"]\n",
    "#     formatted = format_example_from_ner(ner_text, dir_text)\n",
    "#     examples.append(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:25:15.412414Z",
     "iopub.status.busy": "2025-06-02T21:25:15.412087Z",
     "iopub.status.idle": "2025-06-02T21:25:15.430818Z",
     "shell.execute_reply": "2025-06-02T21:25:15.430350Z",
     "shell.execute_reply.started": "2025-06-02T21:25:15.412392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 3. Split into train/validation\n",
    "# # -----------------------------\n",
    "# random.seed(42)\n",
    "# random.shuffle(examples)\n",
    "\n",
    "# split_idx = int(0.9 * len(examples))\n",
    "# train_texts = examples[:split_idx]\n",
    "# val_texts = examples[split_idx:]\n",
    "\n",
    "# # Write out train and validation files\n",
    "# with open(TRAIN_FILE, \"w\", encoding=\"utf-8\") as f_train:\n",
    "#     for ex in train_texts:\n",
    "#         f_train.write(ex)\n",
    "\n",
    "# with open(VAL_FILE, \"w\", encoding=\"utf-8\") as f_val:\n",
    "#     for ex in val_texts:\n",
    "#         f_val.write(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:25:15.432474Z",
     "iopub.status.busy": "2025-06-02T21:25:15.431657Z",
     "iopub.status.idle": "2025-06-02T21:25:23.229367Z",
     "shell.execute_reply": "2025-06-02T21:25:23.228479Z",
     "shell.execute_reply.started": "2025-06-02T21:25:15.432446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing examples:   0%|          | 14000/27071072 [00:01<54:17, 8306.68 examples/s] Token indices sequence length is longer than the specified maximum sequence length for this model (1140 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing examples: 100%|██████████| 27071072/27071072 [52:38<00:00, 8569.62 examples/s] \n",
      "Tokenizing examples: 100%|██████████| 3006866/3006866 [05:52<00:00, 8537.81 examples/s]\n",
      "Grouping into blocks of 1024:  67%|██████▋   | 18194000/27071072 [19:43<09:31, 15528.17 examples/s]"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# -----------------------------\n",
    "# 4. Load GPT-2 tokenizer/model and prepare a Dataset\n",
    "# -----------------------------\n",
    "MODEL_NAME = \"gpt2\"  # or \"gpt2-medium\", \"gpt2-large\" if you have enough VRAM\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model     = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# (Optional) If you wish to add a pad token or any special tokens, do it here:\n",
    "# tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Use the Hugging Face datasets library to load the text files\n",
    "raw_datasets = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": TRAIN_FILE,\n",
    "        \"validation\": VAL_FILE\n",
    "    }\n",
    ")\n",
    "# raw_datasets = {\"train\": Dataset, \"validation\": Dataset}\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # `examples[\"text\"]` is a list of training/validation examples (strings)\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        return_attention_mask=True,\n",
    "        return_special_tokens_mask=False,\n",
    "    )\n",
    "\n",
    "# Apply tokenization in batches\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    desc=\"Tokenizing examples\",\n",
    ")\n",
    "\n",
    "# Now group the tokenized examples into blocks of `block_size` (1024)\n",
    "block_size = 1024\n",
    "\n",
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    Concatenate all input_ids in a batch, then split into chunks of length block_size.\n",
    "    Also create `labels` that are identical to `input_ids` (for causal LM).\n",
    "    \"\"\"\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = (len(concatenated[\"input_ids\"]) // block_size) * block_size\n",
    "\n",
    "    # Drop the remainder if it doesn’t fit evenly\n",
    "    result = {}\n",
    "    for k in concatenated.keys():\n",
    "        chunks = [\n",
    "            concatenated[k][i : i + block_size]\n",
    "            for i in range(0, total_length, block_size)\n",
    "        ]\n",
    "        result[k] = chunks\n",
    "\n",
    "    # Set labels = input_ids for next‐token prediction\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Group and reorder datasets\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    remove_columns=tokenized_datasets[\"train\"].column_names,\n",
    "    desc=f\"Grouping into blocks of {block_size}\",\n",
    ")\n",
    "\n",
    "# Now lm_datasets[\"train\"] and lm_datasets[\"validation\"] each have columns:\n",
    "#  - input_ids (list of length block_size)\n",
    "#  - attention_mask (list of length block_size)\n",
    "#  - labels (same as input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:32:21.591242Z",
     "iopub.status.busy": "2025-06-02T21:32:21.590509Z",
     "iopub.status.idle": "2025-06-02T21:32:21.628141Z",
     "shell.execute_reply": "2025-06-02T21:32:21.627599Z",
     "shell.execute_reply.started": "2025-06-02T21:32:21.591218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Configure TrainingArguments and Trainer\n",
    "# -----------------------------\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,\n",
    "\n",
    "    # Evaluation and logging\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # Mixed precision if supported\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "\n",
    "    # Logging directory for TensorBoard\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:32:25.918884Z",
     "iopub.status.busy": "2025-06-02T21:32:25.918207Z",
     "iopub.status.idle": "2025-06-02T21:32:42.304759Z",
     "shell.execute_reply": "2025-06-02T21:32:42.303733Z",
     "shell.execute_reply.started": "2025-06-02T21:32:25.918858Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to ./gpt2-ner2directions\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Train and save the model\n",
    "# -----------------------------\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Fine-tuned model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:32:47.857356Z",
     "iopub.status.busy": "2025-06-02T21:32:47.857080Z",
     "iopub.status.idle": "2025-06-02T21:32:47.863670Z",
     "shell.execute_reply": "2025-06-02T21:32:47.862991Z",
     "shell.execute_reply.started": "2025-06-02T21:32:47.857336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Inference: generate directions from a NER list\n",
    "# -----------------------------\n",
    "def generate_directions_from_ner(ner_list, max_new_tokens=150):\n",
    "    \"\"\"\n",
    "    Given a Python list of entity strings (e.g. [\"sugar\", \"flour\", \"eggs\"]),\n",
    "    format the prompt as during training and let GPT-2 generate the “Directions:”.\n",
    "    \"\"\"\n",
    "    prompt_lines = [\"NER:\"]\n",
    "    for ent in ner_list:\n",
    "        prompt_lines.append(f\"- {ent.strip()}\")\n",
    "    prompt_lines.append(\"\")  # blank line\n",
    "    prompt_lines.append(\"Directions:\")\n",
    "    prompt = \"\\n\".join(prompt_lines) + \"\\n\"\n",
    "\n",
    "    # Tokenize with attention_mask\n",
    "    encoding = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "        attention_mask = attention_mask.to(\"cuda\")\n",
    "\n",
    "    # Generate up to max_new_tokens beyond the prompt\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=input_ids.shape[-1] + max_new_tokens,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Strip off the prompt itself; return only generated directions\n",
    "    generated = full_output[len(prompt):].strip()\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T21:32:49.816573Z",
     "iopub.status.busy": "2025-06-02T21:32:49.816301Z",
     "iopub.status.idle": "2025-06-02T21:32:51.708818Z",
     "shell.execute_reply": "2025-06-02T21:32:51.708158Z",
     "shell.execute_reply.started": "2025-06-02T21:32:49.816552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample NER List ===\n",
      "['onion', 'garlic', 'tomatoes', 'olive oil', 'basil', 'salt', 'pepper']\n",
      "\n",
      "=== Generated Directions ===\n",
      "1. Preheat oven to 400 degrees.\n",
      "2. In a large bowl, combine onion, tomatoes, garlic, basil, salt, pepper, and salt. Mix well. Cover with plastic wrap and bake at 350 degrees for 20 minutes or until the tomatoes are golden brown. Remove from the oven and allow to cool completely before serving.\n"
     ]
    }
   ],
   "source": [
    "# Example inference\n",
    "sample_ner = [\"onion\", \"garlic\", \"tomatoes\", \"olive oil\", \"basil\", \"salt\", \"pepper\"]\n",
    "print(\"=== Sample NER List ===\")\n",
    "print(sample_ner)\n",
    "print(\"\\n=== Generated Directions ===\")\n",
    "print(generate_directions_from_ner(sample_ner, max_new_tokens=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2957522,
     "sourceId": 5093016,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
