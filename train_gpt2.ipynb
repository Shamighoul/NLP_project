{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f03753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zagorulia/nlp/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a198f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load and preprocess the CSV\n",
    "# -----------------------------\n",
    "INPUT_CSV_PATH = \"3A2M_EXTENDED.csv\"\n",
    "OUTPUT_DIR = \"./gpt2-ner2directions\"\n",
    "TRAIN_FILE = \"train_ner2dir.txt\"\n",
    "VAL_FILE = \"val_ner2dir.txt\"\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(INPUT_CSV_PATH)\n",
    "\n",
    "# Ensure NER and directions columns exist; drop rows missing either\n",
    "if \"NER\" not in df.columns or \"directions\" not in df.columns:\n",
    "    raise ValueError(\"CSV must contain 'NER' and 'directions' columns.\")\n",
    "df = df.dropna(subset=[\"NER\", \"directions\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4523e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Format examples as text blocks\n",
    "# -----------------------------\n",
    "def format_example_from_ner(ner_text: str, directions_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Combine the NER text and directions text into a single string block:\n",
    "    \n",
    "    NER:\n",
    "    - entity1\n",
    "    - entity2\n",
    "    ...\n",
    "\n",
    "    Directions:\n",
    "    1. step one\n",
    "    2. step two\n",
    "    ...\n",
    "    \n",
    "    Ends with two newlines as a delimiter.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    parts.append(\"NER:\")\n",
    "    # Split NER on commas; adjust if your NER uses newlines or another delimiter\n",
    "    for ent in ner_text.split(\",\"):\n",
    "        ent = ent.strip()\n",
    "        if ent:\n",
    "            parts.append(f\"- {ent}\")\n",
    "    parts.append(\"\")  # blank line between sections\n",
    "    parts.append(\"Directions:\")\n",
    "    # Split directions on newline; keep existing multi‐line steps\n",
    "    for idx, step in enumerate(directions_text.split(\"\\n\"), start=1):\n",
    "        step = step.strip()\n",
    "        if step:\n",
    "            parts.append(f\"{idx}. {step}\")\n",
    "    # Two newlines to separate examples\n",
    "    return \"\\n\".join(parts) + \"\\n\\n\"\n",
    "\n",
    "# Build a list of formatted examples\n",
    "examples = []\n",
    "for _, row in df.iterrows():\n",
    "    ner_text = row[\"NER\"]\n",
    "    dir_text = row[\"directions\"]\n",
    "    formatted = format_example_from_ner(ner_text, dir_text)\n",
    "    examples.append(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d8cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Split into train/validation\n",
    "# -----------------------------\n",
    "random.seed(42)\n",
    "random.shuffle(examples)\n",
    "\n",
    "split_idx = int(0.9 * len(examples))\n",
    "train_texts = examples[:split_idx]\n",
    "val_texts = examples[split_idx:]\n",
    "\n",
    "# Write out train and validation files\n",
    "with open(TRAIN_FILE, \"w\", encoding=\"utf-8\") as f_train:\n",
    "    for ex in train_texts:\n",
    "        f_train.write(ex)\n",
    "\n",
    "with open(VAL_FILE, \"w\", encoding=\"utf-8\") as f_val:\n",
    "    for ex in val_texts:\n",
    "        f_val.write(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77af2e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zagorulia/nlp/venv/lib/python3.12/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_text_dataset\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m, tokenizer, block_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m1024\u001b[39m):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TextDataset(\n\u001b[32m     19\u001b[39m         tokenizer=tokenizer,\n\u001b[32m     20\u001b[39m         file_path=file_path,\n\u001b[32m     21\u001b[39m         block_size=block_size,\n\u001b[32m     22\u001b[39m         overwrite_cache=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     23\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m train_dataset = \u001b[43mload_text_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m val_dataset = load_text_dataset(VAL_FILE, tokenizer, block_size=\u001b[32m1024\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Data collator: causal LM (no masking)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mload_text_dataset\u001b[39m\u001b[34m(file_path, tokenizer, block_size)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_text_dataset\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m, tokenizer, block_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m1024\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverwrite_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp/venv/lib/python3.12/site-packages/transformers/data/datasets/language_modeling.py:92\u001b[39m, in \u001b[36mTextDataset.__init__\u001b[39m\u001b[34m(self, tokenizer, file_path, block_size, overwrite_cache, cache_dir)\u001b[39m\n\u001b[32m     89\u001b[39m tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokenized_text) - block_size + \u001b[32m1\u001b[39m, block_size):  \u001b[38;5;66;03m# Truncate in block of block_size\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28mself\u001b[39m.examples.append(\n\u001b[32m     93\u001b[39m         tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n\u001b[32m     94\u001b[39m     )\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Note that we are losing the last truncated example here for the sake of simplicity (no padding)\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# If your dataset is small, first you should look for a bigger one :-) and second you\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# can change this behavior by adding (model specific) padding.\u001b[39;00m\n\u001b[32m     99\u001b[39m start = time.time()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. Load GPT-2 tokenizer/model and create datasets\n",
    "# -----------------------------\n",
    "MODEL_NAME = \"gpt2\"  # or \"gpt2-medium\", \"gpt2-large\" if you have enough VRAM\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# (Optional) If you wish to add a pad token or any special tokens, do it here:\n",
    "# tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "# and then resize the model embeddings after loading the model.\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# If special tokens were added, uncomment:\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Build TextDataset for training and validation\n",
    "def load_text_dataset(file_path: str, tokenizer, block_size: int = 1024):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size,\n",
    "        overwrite_cache=True\n",
    "    )\n",
    "\n",
    "train_dataset = load_text_dataset(TRAIN_FILE, tokenizer, block_size=1024)\n",
    "val_dataset = load_text_dataset(VAL_FILE, tokenizer, block_size=1024)\n",
    "\n",
    "# Data collator: causal LM (no masking)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Configure TrainingArguments and Trainer\n",
    "# -----------------------------\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,\n",
    "\n",
    "    # Evaluation and logging\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # Mixed precision if supported\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "\n",
    "    # Logging directory for TensorBoard\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc66b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Train and save the model\n",
    "# -----------------------------\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Fine-tuned model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Inference: generate directions from a NER list\n",
    "# -----------------------------\n",
    "def generate_directions_from_ner(ner_list, max_new_tokens=150):\n",
    "    \"\"\"\n",
    "    Given a Python list of entity strings (e.g. [\"sugar\", \"flour\", \"eggs\"]),\n",
    "    format the prompt as during training and let GPT-2 generate the “Directions:”.\n",
    "    \"\"\"\n",
    "    prompt_lines = [\"NER:\"]\n",
    "    for ent in ner_list:\n",
    "        prompt_lines.append(f\"- {ent.strip()}\")\n",
    "    prompt_lines.append(\"\")  # blank line\n",
    "    prompt_lines.append(\"Directions:\")\n",
    "    prompt = \"\\n\".join(prompt_lines) + \"\\n\"\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "    # Generate up to max_new_tokens beyond the prompt\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[-1] + max_new_tokens,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Strip off the prompt itself; return only generated directions\n",
    "    generated = full_output[len(prompt):].strip()\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ner = [\"onion\", \"garlic\", \"tomatoes\", \"olive oil\", \"basil\", \"salt\", \"pepper\"]\n",
    "print(\"=== Sample NER List ===\")\n",
    "print(sample_ner)\n",
    "print(\"\\n=== Generated Directions ===\")\n",
    "print(generate_directions_from_ner(sample_ner, max_new_tokens=120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
